== Metrics and Labels

The set of metric labels for logging is described here.

=== Prometheus standard labels ===

All metrics have these standard labels added automatically by prometheus:

* `endpoint`: "web" for HTTP scrape endpoints.
* `instance`: address of scrape endpoint in the form "<ip-literal>:<port>"
* `job`: arbitrary string name to identify related endpoints (e.g. "log_collector")

=== Labels identifying a container ===

Metrics associated with a _Pod_ get the following labels:

* `namespace`: namespace name
* `pod`: pod name
* `uid`: pod UUID
* `node`: node name, as returned by `oc get node -o=jsonpath='{@.items[*].metadata.name}` +
  **Note**: this is node [underline]#resource# name.
  It may, or may not, coincide with the host name, DNS name, or IP address.

Metrics that are associated with a _container_ also get this label:

* `container`: container name

For example, the following metrics are associated with container logs, and have all the above labels:

* `log_logged_bytes_total` provided by a separate agent that watches writes to log files.
* `log_collected_bytes_total` provided by the collector

These labels are compatible with `kubelet`, here's an example `kubelet` metric:

[source,]
----
# HELP kubelet_container_log_filesystem_used_bytes [ALPHA] Bytes used by the container's logs on the filesystem.
# TYPE kubelet_container_log_filesystem_used_bytes gauge
kubelet_container_log_filesystem_used_bytes{container="authentication-operator",namespace="openshift-authentication-operator",pod="authentication-operator-67c88594b5-zftcn ",uid="ead91de5-5e10-42b9-8ab9-6386f21cd554"} 3.44064e+07
----

WARNING: Should we follow `kubelet` and add `_container_` to our metric names? +
`log_container_logged_bytes_total` `log_container_collected_bytes_total`

=== Label identifying a cluster

Multi-cluster deployments need a `cluster` label
This label will not be provided by every metric endpoints, it can be added by  prometheus cluster-wide.

NOTE: There is a lack of clarity about a standard unique identifier for a cluster.
Our preferred option is the DNS authority (`host:port` part) of the public DNS name for the API server.

We will proceed on this assumption, but to be aware of the debate:

* DNS authority
** [x] Easy to read/remember
** [x] Guaranteed network-unique, globally unique for a registered DNS domain.
** [x] Definitely known to anyone with API access \
   `oc config view -o jsonpath='{.clusters[ ].name}{"\n"}'`
* UUID
** [x] Guaranteed unique
** [ ] Hard to read and remember
** [ ] Need to generate and discover it somehow
* Manually chosen string:
** [x] Easy to read/remember
** [ ] Not suitable for large multi-cluster, need automatic ID
** [ ] Not guaranteed unique

Relevant discussions:

* https://github.com/kubernetes/kubernetes/issues/2292[UUID for a cluster · Issue #2292 · kubernetes/kubernetes]
* https://github.com/openshift/hive/issues/106[Clarify Cluster ID, UUID, Name in API · Issue #106 · openshift/hive]
* https://github.com/openshift/origin/issues/10682[How do I check my Kubernets Master URL? · Issue #10682 · openshift/origin]
